## ebeans

### "Model not enhanced"

`ebeans` generates code as part of the [ORM](https://en.wikipedia.org/wiki/Object%E2%80%93relational_mapping).  The full object, including the generated code, is referred to as "enhanced".  The generated object is injected using Guice into some database machinery.  Other terms for "enhancement" include "weaving", "transformation", and "byte code manipulation" [ref](https://ebean.io/docs/setup/enhancement).

If something goes wrong with this process, you may see an error in tests at the time that the database connection is being initiated.  It will use the words "not enhanced", and contain a list of troubleshooting steps.  The error will tell you to [install a plugin for IDEA or Eclipse](https://ebean.io/docs/trouble-shooting).  This is a red herring!  We do ebeans enhancement at build time, and that plugin is for projects which don't use a project management tool (like `sbt` or `mvn`).  The plugin performs a small portion of what `sbt` does, to enable you to click the green "build" button in your IDE - but none of the rest of our tooling will work with that, so there's no benefit, no need to do this.

The error will also tell you to check `ebeans.mf`.  We don't have that - that file is automatically generated by the play integration.  Our equivalent of that is in `conf/application.conf`, near the bottom, `ebeans.default = "models.*"`.  This step is worth doing - is your model outside of `models.*`?  If so, add your model to the list.  This is just one of the ways that the framework is highly dependent on our particular package structure, which should not be changed without good reason - you might find that strange things start happening.

The most likely cause of this issue is actually an apparently-benign issue loading the ebeans code.  The `sbt test` / `sbt run` will continue, but at the top of the output, you might see `java.lang.UnsupportedClassVersionError: Unsupported major version 59`.  This means that you've compiled the code with a higher version of the jdk than you're running it with - at *some point*.  It's not guaranteed that you're currently trying to do that - `sbt` tries to be really smart about incremental compilation, so you may have compiled it by, e.g. accidentally hitting the "build" button in Eclipse from muscle memory, possibly quite some time ago.

The best solution to this is to run `sbt clean cleanFiles`.  This is not quite a full clean - you must then delete every directory named `target` under the project root (at minimum `target/`, `project/target/`, and `project/project/target/`), then re-run a docker build / test.

## sbt

### IOError - could not create directory, `_global/inputStreams`

(this answer is speculative - constructed from memory - edit if you have confidence that another answer is correct)

`sbt` will spew the same error repeatedly in a tight retry loop - it's trying to create an output directory structure, and it can't do that.  This error indicates that you have two `sbt` commands running at the same time in the same project.  It can be hard to find the other one, if for instance you've launched it in IntelliJ where it seems to exit but does not.  Find the shell where `sbt` is running, and type `exit` (or just find the process using `ps` and `kill` it).

## docker

### The docker container is not picking up my changes,
The docker container no longer requires the volume mount, so it could be that you ran the container without the volume mapping.  Try adding `-v $PWD/universal-application-tool-0.0.1:/usr/src/universal-application-tool-0.0.1`.  If you're running with `docker-compose` or `bin/run-dev`, check that you're running the command from the correct directory (the root of the git repo, one directory above the `build.sbt` file).

### The tests are failing since Docker is not available inside the container.
The tests run a postgres docker container for full integration testing.  We used to use an in memory database (which play supports quite well), but we stopped doing that because the in-memory database doesn't support the json types we are using as the core of our application, and we would have had to mock out the majority of our database interactions even in the integration tests.  However, this means that the tests (which run inside a docker container) need to be able to start another docker container.  This situation is called "docker in docker", and [everyone recommends against doing it](https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/) (the author of that article is the one who invented the tools to make it possible).

Instead, we cheat a little.  The docker client communicates with the docker daemon using a unix socket located at `/var/run/docker.sock`.  So, we just mount the host machine's `/var/run/docker.sock` inside the container's `/var/run`, the container's docker client communicates with the host system's docker daemon, and everything works fine.  Add the magic string `-v /var/run/docker.sock:/var/run/docker.sock` to the `docker run` invocation.

If you find yourself needing to do this during a `docker build` command - for instance because you'd like to run unit tests during container build, as we used to be able to do - you're not likely to succeed and I'd recommend finding another way to do what you need to do.  Volume mounts (which allow this end-running of docker-in-docker) are not available at container build time.